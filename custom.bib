% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{wei2022chain,
  author= {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal= {Conference on Neural Information Processing Systems},
  title={Chain of thought prompting elicits reasoning in large language models},
  year={2022},
  url={https://arxiv.org/pdf/2201.11903.pdf}
}

@article{Wang2022,
  author= {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed H. and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal= {ICLR 2023},
  title={SELF CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS},
  year={2023},
  url={https://arxiv.org/pdf/2203.11171.pdf}
}

@article{Elazar2021,
  title={Measuring and Improving Consistency in Pretrained Language Models},
  author= {Elazar, Yanai and Kassner, Nora and Ravfogel1, Shauli and Ravichander, Abhilasha and Hovy, Eduard and Schutze, Hinrich and Goldberg, Yoav},
  journal= {TACL journal 2021},
  year={2021},
  url={https://arxiv.org/pdf/2102.01017.pdf}
}

@article{chai_human_2023,
  title={Human-in-the-Loop through Chain-of-Thought},
  author= {Cai1, Zefan and Chang1, Baobao and  Han, Wenjuan},
  journal= {arXiv:2306.07932v2 [cs.CL] 23 Jun 2023},
  year={2023},
  url={https://arxiv.org/pdf/2306.07932.pdf}
}

@article{de-or-ec-2023,
  title={Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder},
  author= {Fu, Zihao and Lam, Wai and Yu, Qian and Man-Cho So, Anthony and Hu, Shengding and Liu, Zhiyuan and Collier, Nigel},
  journal= {arXiv:2304.04052},
  year={2023},
  url={https://arxiv.org/pdf/2304.04052.pdf}
}

@article{trainig-ver-cobbe,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={rXiv:2110.14168v2 [cs.LG] 18 Nov 2021},
  year={2021},
  url={https://arxiv.org/pdf/2110.14168.pdf}
}

@article{multimod-zhang-2023,
  title={Multimodal Chain-of-Thought Reasoning in Language Models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  journal={arXiv:2302.00923v4 [cs.CL] 17 Feb 2023},
  year={2023},
  url={https://arxiv.org/pdf/2302.00923.pdf}
}

@article{conc-rnn-welleck,
  title={Consistency of a Recurrent Language Model With Respect to Incomplete Decoding},
  author= {Welleck, Sean and Kulikov, Ilia and Kim, Jaedeok and Pang, Richard Yuanzhe and Cho, Kyunghyun},
  journal= {10.18653/v1/2020.emnlp-main.448},
  year={2020},
  url={https://aclanthology.org/2020.emnlp-main.448.pdf}
}

@article{unr-fsp-2022,
  title={The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning},
  author= {Ye, Xi and Durrett, Greg},
  journal= {NeurIPS 2022 Conference Program Chairs},
  year={2022},
  url={https://openreview.net/pdf?id=Bct2f8fRd8S}
}

@article{lm-zsl-2020-brown,
  title={Language Models are Few-Shot Learners},
  author= {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario.},
  journal= {arXiv:2005.14165},
  year={2020},
  url={https://arxiv.org/pdf/2005.14165.pdf}
}

@article{palm-2022,
  title={PaLM: Scaling Language Modeling with Pathways},
  author= {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  journal= {rXiv:2204.02311v5 [cs.CL] 5 Oct 2022},
  year={2022},
  url={https://arxiv.org/pdf/2204.02311.pdf}
}

@article{mut-inf-2016,
  title={Mutual Information and Diverse Decoding Improve Neural Machine
Translation},
  author= {Li, Jiwei and Jurafsky, Dan},
  journal= {arXiv:1601.00372v2 [cs.CL] 22 Mar 2016},
  year={2016},
  url={https://arxiv.org/pdf/1601.00372.pdf}
}


@article{palm-2020,
  title={PALM: Pre-training an Autoencoding&Autoregressive Language Model
for Context-conditioned Generation},
  author= {Bi, Bin and Li, Chenliang and Wu, Chen and Yan, Ming and Wang, Wei and Huang, Songfang and Fei, Huang and Si, Luo},
  journal= {rXiv:2004.07159v2 [cs.CL] 20 Sep 2020},
  year={2023},
  url={https://arxiv.org/pdf/2004.07159.pdf}
}

@article{llm-clinical-2023,
  title={Large language models encode clinical knowledge},
  author= {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly, Chris and Babiker, Abubakr and Schärli, Nathanael and Chowdhery, Aakanksha and Mansfield, Philip and Demner-Fushman, Dina and Agüera y Arcas, Blaise and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Chou, Katherine and Natarajan, Vivek},
  journal= {10.1038/s41586-023-06291-2},
  year={2023},
  url={https://www.nature.com/articles/s41586-023-06291-2}
}

@article{coh-cons-nsm-2021,
  title={Improving Coherence and Consistency in Neural Sequence Models with Dual-System, Neuro-Symbolic Reasoning},
  author= {Nye, Maxwell and Tessler, Michael Henry and Tenenbaum, Joshua B. and Lake, Brenden M.},
  journal= {NeurIPS 2021},
  year={2020},
  url={https://openreview.net/pdf?id=uyKk_avJ-p4}
}

@article{muster,
  title={},
  author= {},
  journal= {},
  year={},
  url={}
}

@inproceedings{roy-roth-2015-solving,
    title = "Solving General Arithmetic Word Problems",
    author = "Roy, Subhro  and
      Roth, Dan",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1202",
    doi = "10.18653/v1/D15-1202",
    pages = "1743--1752",
}

@article{MWPToolkit,
  author       = {Yihuai Lan and
                  Lei Wang and
                  Qiyuan Zhang and
                  Yunshi Lan and
                  Bing Tian Dai and
                  Yan Wang and
                  Dongxiang Zhang and
                  Ee{-}Peng Lim},
  title        = {MWPToolkit: An Open-Source Framework for Deep Learning-Based Math
                  Word Problem Solvers},
  journal      = {CoRR},
  volume       = {abs/2109.00799},
  year         = {2021},
  url          = {https://arxiv.org/abs/2109.00799},
  eprinttype    = {arXiv},
  eprint       = {2109.00799},
  timestamp    = {Tue, 15 Nov 2022 17:56:35 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2109-00799.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{amini-etal-2019-mathqa,
    title = "{M}ath{QA}: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
    author = "Amini, Aida  and
      Gabriel, Saadia  and
      Lin, Shanchuan  and
      Koncel-Kedziorski, Rik  and
      Choi, Yejin  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1245",
    doi = "10.18653/v1/N19-1245",
    pages = "2357--2367",
    abstract = "We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver by learning to map problems to their operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, we significantly enhance the AQUA-RAT dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model with automatic problem categorization. Our experiments show improvements over competitive baselines in our dataset as well as the AQUA-RAT dataset. The results are still lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at https://math-qa.github.io/math-QA/",
}

@misc{pi2022reasoning,
      title={Reasoning Like Program Executors}, 
      author={Xinyu Pi and Qian Liu and Bei Chen and Morteza Ziyadi and Zeqi Lin and Qiang Fu and Yan Gao and Jian-Guang Lou and Weizhu Chen},
      year={2022},
      eprint={2201.11473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{hum-par-2021,
  author       = {Yichong Xu and
                  Chenguang Zhu and
                  Shuohang Wang and
                  Siqi Sun and
                  Hao Cheng and
                  Xiaodong Liu and
                  Jianfeng Gao and
                  Pengcheng He and
                  Michael Zeng and
                  Xuedong Huang},
  title        = {Human Parity on CommonsenseQA: Augmenting Self-Attention with External
                  Attention},
  journal      = {CoRR},
  volume       = {abs/2112.03254},
  year         = {2021},
  url          = {https://arxiv.org/abs/2112.03254},
  eprinttype    = {arXiv},
  eprint       = {2112.03254},
  timestamp    = {Mon, 30 May 2022 13:48:57 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2112-03254.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{khashabi-etal-2020-unifiedqa,
    title = "{UNIFIEDQA}: Crossing Format Boundaries with a Single {QA} System",
    author = "Khashabi, Daniel  and
      Min, Sewon  and
      Khot, Tushar  and
      Sabharwal, Ashish  and
      Tafjord, Oyvind  and
      Clark, Peter  and
      Hajishirzi, Hannaneh",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.171",
    doi = "10.18653/v1/2020.findings-emnlp.171",
    pages = "1896--1907",
    abstract = "Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats. UNIFIEDQA performs on par with 8 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UNIFIEDQA performs surprisingly well, showing strong generalization from its outof-format training data. Finally, simply finetuning this pre trained QA model into specialized models results in a new state of the art on 10 factoid and commonsense question answering datasets, establishing UNIFIEDQA as a strong starting point for building QA systems.",
}